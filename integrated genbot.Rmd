---
title: "COMP440 Final Project: Genbot"
author: "Amber Wetzel and Lang Si"
date: "April 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
#Must first install keras/tensorflow like so:
# https://www.tensorflow.org/versions/master/install/install_windows
#assumes you downloaded via anaconda
#type "activate r-tensorflow" into anaconda terminal before running

#ONLY USE COMMENTED LINES THE FIRST TIME YOU USE THIS CODE
#install.packages("keras")
suppressPackageStartupMessages(library(keras))
#install_keras(method="conda")

suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(tokenizers))

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidytext))
suppressPackageStartupMessages(library(httpuv))
suppressPackageStartupMessages(library(twitteR))
suppressPackageStartupMessages(library(rtweet))
suppressPackageStartupMessages(library(qdapRegex))
```

## Setup

```{r}

appname <- "get from apps.twitter"
api_key <- "get from apps.twitter"
api_secret <- "get from apps.twitter"
access_token <- "get from apps.twitter"
access_token_secret <- "get from apps.twitter"
options(httr_oauth_cache = TRUE)

#authorize for twitteR (works for rtweet also)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

```


```{r}
##Step 0: Follow anyone who has followed genbot so message requests are accepted
#Uses rtweet code instead of twitteR
followers <- get_followers("tweetgenbot", n = 25, page = "-1", retryonratelimit = FALSE, parse = TRUE, verbose = FALSE, token = NULL)
following <- get_friends("tweetgenbot", n = 25, retryonratelimit = FALSE, page = "-1", parse = TRUE, verbose = FALSE, token = NULL)
#reduce followers to only new followers
followers <- followers[!(followers$user_id %in% following$user_id),]

if(length(followers$user_id)!=0){
    for (i in 1:length(followers$user_id)) {
      post_follow(followers$user_id[i], destroy = FALSE, mute = FALSE, notify = FALSE, retweets = TRUE, token = NULL)
  }
}

```

```{r}
#For model later
maxlen <- 3  #This parameter controls the number of words the model puts in, in this case, the model considered all tri-grams.

#Thanks to Taylor Grant for interactive twitterbot guide https://rstudio-pubs-static.s3.amazonaws.com/132986_5fbf5123bfcb452998d7ff1d726410c1.html

##COMMENT THIS OUT AFTER FIRST USE
# genesis for data ID
#dms <- dmGet()
#x <- dms[[1]]
#id1 <- data.frame(id1 = as.numeric(x$getId()))
#write_tsv(id1, "id1.tsv")
# read in current tweetID of last DM responded to

##Step 1: recieve and clean the input
curr_id <- as.numeric(read_table("id1.tsv"))

# establish search words worth replying to
## this is spelled out specifically in the bot account's profile
words <- c('tweet about')

# gather dms and pull out unique tweetIDs
dms <- dmGet(sinceID = curr_id)
test_id <- sapply(dms, function(x) as.vector(as.numeric(x$getId())))

# test if any DMs are new compared to benchmark
new <- length(test_id[test_id > curr_id])

if (new == 0) {
  NULL
}
if (new > 0) {
  # pull out screen_name of sender and tweet text
  name <- sapply(dms, function(x) as.vector(x$getSenderSN()))[c(seq(new))]
  text <- sapply(dms, function(x) as.vector(x$getText()))[c(seq(new))]
  test <- data.frame(name,text)
  
  # test to see if DM matches our search phrase, keep those names that do
  final <- subset(test, grepl(words, test$text, fixed=TRUE))
  
  # searching and generating tweet
  if (length(final$name) > 0) {
    
    # make sure not sending multiple DMs to same account
    final <- final[!duplicated(final$name),]
   
     # send DM response to each account
    for (i in 1:length(final$name)) {
      ##Step 2: get search term and search on it
      search <- sub('.*tweet about\\s*', '', tolower(final$text[i]))
      data_df <- search_tweets(search, n = 3000, include_rts = FALSE)

      ##Step 3: Clean the search results (get rid of @'s, links, pictures)
      #filter for english tweets
      data_df <- subset(data_df,data_df$lang=="en")
      #remove urls (also removes pictures)
      data_df$text <- rm_url(data_df$text)
      #remove hashtags and @'s
      data_df$text <- gsub("[#@]\\S+", '',data_df$text)
      #remove emojis and bizzare signs
      data_df$text <- iconv(data_df$text, from = "latin1", to = "ASCII", sub="")
      #remove unneccessary whitespace
      data_df$text <- gsub("[ \t]{2,}", " ", data_df$text)
      data_df$text <- gsub( "^\\s+|\\s+$", "", data_df$text)
      #remove tweets with only whitespace
      data_df <- data_df[!grepl("^\\s*$", data_df$text),]
      #remove duplicates
      data_df <- data_df[!duplicated(data_df$text),]

      #Write text to .txt file so model can use it- rerun every time
      write.table(data_df$text, file = "tweetText.txt", sep = "\t", quote = FALSE,
            row.names = FALSE)
    
      ##Step 4: generate the tweet
      #Thanks to tensorflow documentation for the base
      # https://tensorflow.rstudio.com/keras/articles/examples/lstm_text_generation.html
      
      # Data Preparation --------------------------------------------------------
      path <- read_file('tweetText.txt')

      # Load, collapse, and tokenize text
      text <- read_lines(path) %>%
        str_to_lower() %>%
        str_c(collapse = "\n") %>%
        tokenize_words( simplify = TRUE) # Tokenize the text into words.
      
      chars <- text %>% 
        unique() %>%
        sort() #Find unique words
      
      # Cut the text in semi-redundant sequences of maxlen characters
      dataset <- map(
        seq(1, length(text) - maxlen - 1, by = 3), 
        ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
        ) #Map the data set containing all the bigrams and the next word after that.
      
      dataset <- transpose(dataset)
      
      # Vectorization
      X <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars))) #Construct a 3D array
      y <- array(0, dim = c(length(dataset$sentece), length(chars))) #2D array for 
      
      # This for loop basically construct a machine learning model so that it can predict the next word appears after the trigram.
      for(i in 1:length(dataset$sentece)){
        
        X[i,,] <- sapply(chars, function(x){ #3D array, put all the trigrams into X
          as.integer(x == dataset$sentece[[i]])
        })
        
        y[i,] <- as.integer(chars == dataset$next_char[[i]]) #2D array # Take the next character into y. 
        
      }
      
      # Model Definition --------------------------------------------------------
      
      model <- keras_model_sequential() #Seq2Seq model of LSTM (RNN)
      
      model %>%
        layer_lstm(128, input_shape = c(maxlen, length(chars))) %>% #Initiate a LSTM model with 128 input unit
        layer_dense(length(chars)) %>% #Output units size is the total unique words length
        layer_activation("softmax") #Use softmax function to apply on the output.
      
      optimizer <- optimizer_rmsprop(lr = 0.01) #Optimize the result using RMSProp. Suitable for RNN. 
      
      model %>% compile(
        loss = "categorical_crossentropy", 
        optimizer = optimizer #Using cross entropy to calculate loss.
      )
      
      # Training & Results ----------------------------------------------------
      
      sample_mod <- function(preds, temperature = 1){
        preds <- log(preds)/temperature
        exp_preds <- exp(preds)
        preds <- exp_preds/sum(exp(preds))
        
        rmultinom(1, 1, preds) %>% 
          as.integer() %>%
          which.max()
      }
      
      for(iteration in 1:40){
        
        cat(sprintf("iteration: %02d ---------------\n\n", iteration))
        
        model %>% fit(
          X,y,
          batch_size = 128,
          epochs = 1
        )
        
     
          
          cat(sprintf("diversity: %f ---------------\n\n", 0.2))
          
          start_index <- sample(1:(length(text) - maxlen), size = 1)
          sentence <- text[start_index:(start_index + maxlen - 1)]
          generated <- ""
          
          for(i in 1:20){
            
            x <- sapply(chars, function(x){
              as.integer(x == sentence)
            })
            x <- array_reshape(x, c(1, dim(x)))
            
            preds <- predict(model, x)
            next_index <- sample_mod(preds, 0.2)
            next_char <- chars[next_index]
            
            generated <- str_c(generated, next_char, collapse = "")
            generated <-paste(generated," ")
            sentence <- c(sentence[-1], next_char)
            
          }
        }
      }
      
      dmSend(generated, final$name[i])
    }
  
  else if (length(final$name) == 0) {
    NULL
  }
}

# write over the tweetID file with updated tweetID 
tmp_id <- data.frame(test_id[1])
write_tsv(tmp_id, "id1.tsv")

```

