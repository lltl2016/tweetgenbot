---
title: "Training Model"
author: "Lang Li"
date: "April 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is our training model right now: Credit: https://tensorflow.rstudio.com/keras/articles/examples/lstm_text_generation.html
We convert Char RNN to Word RNN by slightly modifying the parameter and explore several different options on Model specification.
```{r}
library(keras)

library(readr)
library(stringr)
library(purrr)
library(tidyverse)
library(tokenizers)
```

```{r}
# Parameters --------------------------------------------------------------
 
maxlen <- 5  #This parameter controls the number of words the model puts in, in this case, the model considered all 5-grams.

# Data Preparation --------------------------------------------------------

# Retrieve text
path <- "C:\\Users\\Lang\\OneDrive\\Macalester\\COMP440\\tweetgenbot\\tweetText.txt" 

# Load, collapse, and tokenize text
text <- read_lines(path) %>%
  str_to_lower() %>%
  str_c(collapse = "\n") %>%
  tokenize_words( simplify = TRUE) # Tokenize the text into words.

chars <- text %>% 
  unique() %>%
  sort() #Find unique words
```

```{r}
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
  seq(1, length(text) - maxlen - 1, by = 3), 
  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
  ) #Map the data set containing all the 5-grams and the next word after that.

dataset <- transpose(dataset)

# Vectorization
X <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars))) #Construct a 3D array
y <- array(0, dim = c(length(dataset$sentece), length(chars))) #2D array for 

# This for loop basically construct a machine learning model so that it can predict the next word appears after the trigram.
for(i in 1:length(dataset$sentece)){
  
  X[i,,] <- sapply(chars, function(x){ #3D array, put all the 5-grams into X
    as.integer(x == dataset$sentece[[i]])
  })
  
  y[i,] <- as.integer(chars == dataset$next_char[[i]]) #2D array # Take the next character into y. 
  
}
```

```{r}
# Model Definition --------------------------------------------------------

model <- keras_model_sequential() #Seq2Seq model of LSTM (RNN)

model %>%
  layer_lstm(512, input_shape = c(maxlen, length(chars))) %>% #Initiate a LSTM model with 128 hidden unit
  layer_dense(length(chars)) %>% #Output units size is the total unique words length
  layer_activation("softmax") #Use softmax function to apply on the output.

optimizer <- optimizer_rmsprop(lr = 0.01) #Optimize the result using RMSProp. Suitable for RNN. 

model %>% compile(
  loss = "categorical_crossentropy", 
  optimizer = optimizer #Using cross entropy to calculate loss.
)
```

```{r}
# Training & Results ----------------------------------------------------

sample_mod <- function(preds, temperature = 1){
  preds <- log(preds)/temperature
  exp_preds <- exp(preds)
  preds <- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %>% 
    as.integer() %>%
    which.max()
}
```

```{r}
for(iteration in 1:60){
  
  cat(sprintf("iteration: %02d ---------------\n\n", iteration))
  
  model %>% fit(
    X,y,
    batch_size = 128,
    epochs = 1
  )
  
  for(diversity in c(0.2, 0.5)){
    
    cat(sprintf("diversity: %f ---------------\n\n", diversity))
    
    start_index <- sample(1:(length(text) - maxlen), size = 1)
    sentence <- text[start_index:(start_index + maxlen - 1)]
    generated <- ""
    
    for(i in 1:20){
      
      x <- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x <- array_reshape(x, c(1, dim(x)))
      
      preds <- predict(model, x)
      next_index <- sample_mod(preds, diversity)
      next_char <- chars[next_index]
      
      generated <- str_c(generated, next_char, collapse = "")
      generated <-paste(generated," ")
      sentence <- c(sentence[-1], next_char)
      
    }
    
    cat(generated)
    cat("\n\n")
    
  }
}
```

